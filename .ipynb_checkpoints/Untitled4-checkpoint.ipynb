{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Shape: (903653, 55)\n",
      "Loaded test.csv. Shape: (804684, 53)\n",
      "['channelGrouping', 'visitNumber', 'device.browser', 'device.deviceCategory', 'device.isMobile', 'device.operatingSystem', 'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent', 'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign', 'trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.medium', 'trafficSource.referralPath', 'trafficSource.source', 'Sessions', 'Avg. Session Duration', 'Bounce Rate', 'Revenue', 'Transactions', 'Goal Conversion Rate', 'sess_date_dow', 'sess_date_hours', 'sess_month', 'hits/pageviews', 'is_high_hits']\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 2.60924\tvalid_0's rmse: 1.61531\n",
      "[200]\tvalid_0's l2: 2.56472\tvalid_0's rmse: 1.60148\n",
      "[300]\tvalid_0's l2: 2.55454\tvalid_0's rmse: 1.59829\n",
      "[400]\tvalid_0's l2: 2.55044\tvalid_0's rmse: 1.59701\n",
      "[500]\tvalid_0's l2: 2.55099\tvalid_0's rmse: 1.59718\n",
      "Early stopping, best iteration is:\n",
      "[408]\tvalid_0's l2: 2.54936\tvalid_0's rmse: 1.59667\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 2.56256\tvalid_0's rmse: 1.6008\n",
      "[200]\tvalid_0's l2: 2.528\tvalid_0's rmse: 1.58997\n",
      "[300]\tvalid_0's l2: 2.51729\tvalid_0's rmse: 1.5866\n",
      "[400]\tvalid_0's l2: 2.51308\tvalid_0's rmse: 1.58527\n",
      "[500]\tvalid_0's l2: 2.51255\tvalid_0's rmse: 1.5851\n",
      "Early stopping, best iteration is:\n",
      "[489]\tvalid_0's l2: 2.51214\tvalid_0's rmse: 1.58497\n",
      "1.5904927653186764\n",
      "(714167, 323)\n",
      "(617242, 323)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's l2: 2.03729\tTRAIN's rmse: 1.42734\tVALID's l2: 2.25429\tVALID's rmse: 1.50143\n",
      "[200]\tTRAIN's l2: 1.88811\tTRAIN's rmse: 1.37409\tVALID's l2: 2.24867\tVALID's rmse: 1.49956\n",
      "Early stopping, best iteration is:\n",
      "[139]\tTRAIN's l2: 1.96158\tTRAIN's rmse: 1.40056\tVALID's l2: 2.24547\tVALID's rmse: 1.49849\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's l2: 2.02266\tTRAIN's rmse: 1.4222\tVALID's l2: 2.24998\tVALID's rmse: 1.49999\n",
      "[200]\tTRAIN's l2: 1.87255\tTRAIN's rmse: 1.36841\tVALID's l2: 2.2529\tVALID's rmse: 1.50097\n",
      "Early stopping, best iteration is:\n",
      "[132]\tTRAIN's l2: 1.95763\tTRAIN's rmse: 1.39915\tVALID's l2: 2.24416\tVALID's rmse: 1.49805\n",
      "1.4982657965267991\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "pd.set_option(\"display.max_rows\", 1000, \"display.max_columns\", 1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#using https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields/notebook\n",
    "def load_df(csv_path='train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'date': str, 'fullVisitorId': str, 'sessionId':str, 'visitId': np.int64},\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df\n",
    "    \n",
    "train = load_df()\n",
    "test = load_df(\"test.csv\")\n",
    "\n",
    "#Loading external data\n",
    "train_store_1 = pd.read_csv('Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "train_store_2 = pd.read_csv('Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_1 = pd.read_csv('Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_2 = pd.read_csv('Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "\n",
    "#Getting VisitId to Join with our train, test data\n",
    "def get_visitid(id):\n",
    "    bef_, af_ = str(x).split('.')\n",
    "    return int(bef_), (int(af_)*10 if len(af_)==1 else int(af_))\n",
    "\n",
    "for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n",
    "    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(str)\n",
    "\n",
    "train_exdata = pd.concat([train_store_1, train_store_2], sort=False)\n",
    "test_exdata = pd.concat([test_store_1, test_store_2], sort=False)\n",
    "\n",
    "for df in [train, test]:\n",
    "    df[\"visitId\"] = df[\"visitId\"].astype(str)\n",
    "\n",
    "# Merge with train/test data\n",
    "train_new = train.merge(train_exdata, how=\"left\", on=\"visitId\")\n",
    "test_new = test.merge(test_exdata, how=\"left\", on=\"visitId\")\n",
    "\n",
    "# Drop Client Id\n",
    "for df in [train_new, test_new]:\n",
    "    df.drop(\"Client Id\", 1, inplace=True)\n",
    "\n",
    "#Cleaning Revenue\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Revenue\"].fillna('$', inplace=True)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n",
    "    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n",
    "    df[\"Revenue\"].fillna(0.0, inplace=True)\n",
    "\n",
    "#Imputing NaN\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Sessions\"] = df[\"Sessions\"].fillna(0)\n",
    "    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].fillna(0)\n",
    "    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].fillna(0)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].fillna(0)\n",
    "    df[\"Transactions\"] = df[\"Transactions\"].fillna(0)\n",
    "    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].fillna(0)\n",
    "    df['trafficSource.adContent'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.slot'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.page'].fillna(0.0, inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.adNetworkType'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.gclId'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.isTrueDirect'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.referralPath'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.keyword'].fillna('N/A', inplace=True)\n",
    "    df['totals.bounces'].fillna(0.0, inplace=True)\n",
    "    df['totals.newVisits'].fillna(0.0, inplace=True)\n",
    "    df['totals.pageviews'].fillna(0.0, inplace=True)\n",
    "\n",
    "del train\n",
    "del test\n",
    "train = train_new\n",
    "test = test_new\n",
    "del train_new\n",
    "del test_new\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['date_new'] = pd.to_datetime(df['visitStartTime'], unit='s')\n",
    "    df['date'] = df['date_new'].dt.date\n",
    "    df['sess_date_dow'] = df['date_new'].dt.dayofweek\n",
    "    df['sess_date_hours'] = df['date_new'].dt.hour\n",
    "    df['sess_month'] = df['date_new'].dt.month\n",
    "\n",
    "# Dropping Constant Columns\n",
    "\n",
    "const_cols = [col for col in train.columns if len(train[col].unique())==1]\n",
    "\n",
    "for df in [train, test]:\n",
    "    df.drop(const_cols, 1, inplace=True)\n",
    "\n",
    "train.drop('trafficSource.campaignCode', 1, inplace=True)\n",
    "\n",
    "# Modeling with Olivier's method\n",
    "#https://www.kaggle.com/ogrellier/teach-lightgbm-to-sum-predictions\n",
    "\n",
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids\n",
    "\n",
    "y_reg = train['totals.transactionRevenue'].fillna(0)\n",
    "del train['totals.transactionRevenue']\n",
    "\n",
    "if 'totals.transactionRevenue' in test.columns:\n",
    "    del test['totals.transactionRevenue']\n",
    "\n",
    "excluded_features = [\n",
    "    'date', 'date_new', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n",
    "    'visitId', 'visitStartTime'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    _f for _f in train.columns\n",
    "    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n",
    "]\n",
    "\n",
    "for f in categorical_features:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['hits/pageviews'] = (df[\"totals.pageviews\"]/(df[\"totals.hits\"])).apply(lambda x: 0 if np.isinf(x) else x)\n",
    "    df['is_high_hits'] = np.logical_or(df[\"totals.hits\"]>4,df[\"totals.pageviews\"]>4).astype(np.int32)\n",
    "    df[\"Revenue\"] = np.log1p(df[\"Revenue\"])\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "folds = get_folds(df=train, n_splits=2) #changed from 5\n",
    "y_reg = y_reg.astype(float)\n",
    "train_features = [_f for _f in train.columns if _f not in excluded_features]\n",
    "print(train_features)\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "oof_reg_preds = np.zeros(train.shape[0])\n",
    "sub_reg_preds = np.zeros(test.shape[0])\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n",
    "    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = train_features\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_reg_preds[oof_reg_preds < 0] = 0\n",
    "    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_reg_preds += np.expm1(_preds) / len(folds)\n",
    "    \n",
    "print(mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5)\n",
    "\n",
    "\n",
    "# importances['gain_log'] = np.log1p(importances['gain'])\n",
    "# mean_gain = importances[['gain', 'feature']].groupby('feature').mean()\n",
    "# importances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "# plt.figure(figsize=(8, 12))\n",
    "# plot = sns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))\n",
    "# fig = plot.get_figure()\n",
    "# fig.savefig(\"feat_imp_1.png\")\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "train['predictions'] = np.expm1(oof_reg_preds)\n",
    "test['predictions'] = sub_reg_preds\n",
    "\n",
    "# Aggregate data at User level\n",
    "trn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "\n",
    "# Create a list of predictions for each Visitor\n",
    "trn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "# Create a DataFrame with VisitorId as index\n",
    "# trn_pred_list contains dict \n",
    "# so creating a dataframe from it will expand dict values into columns\n",
    "trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\n",
    "trn_feats = trn_all_predictions.columns\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\n",
    "    trn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\n",
    "    trn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\n",
    "    trn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "    trn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "    full_data = pd.concat([trn_data, trn_all_predictions], axis=1)\n",
    "    del trn_data, trn_all_predictions\n",
    "    gc.collect()\n",
    "    print(full_data.shape)\n",
    "\n",
    "\n",
    "sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for f in trn_feats:\n",
    "        if f not in sub_all_predictions.columns:\n",
    "            sub_all_predictions[f] = np.nan\n",
    "    sub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\n",
    "    sub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\n",
    "    sub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\n",
    "    sub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "    sub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "    sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\n",
    "    del sub_data, sub_all_predictions\n",
    "    gc.collect()\n",
    "    print(sub_full_data.shape)\n",
    "\n",
    "# Create target at Visitor level\n",
    "train['target'] = y_reg\n",
    "trn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()\n",
    "\n",
    "# Train a model at Visitor level\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "folds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=2)#changed from 5\n",
    "\n",
    "oof_preds = np.zeros(full_data.shape[0])\n",
    "sub_preds = np.zeros(sub_full_data.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=2000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        eval_names=['TRAIN', 'VALID'],\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = trn_x.columns\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_preds[oof_preds < 0] = 0\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_preds += _preds / len(folds)\n",
    "    \n",
    "print(mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5)\n",
    "\n",
    "# # Display feature importances\n",
    "# vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\n",
    "# mean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\n",
    "# vis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "# plt.figure(figsize=(8, 25))\n",
    "# plot = sns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])\n",
    "# fig = plot.get_figure()\n",
    "# fig.savefig(\"feat_imp_2.png\")\n",
    "# plt.close()\n",
    "\n",
    "# Save predictions\n",
    "sub_full_data['PredictedLogRevenue'] = sub_preds\n",
    "sub_full_data[['PredictedLogRevenue']].to_csv('submission_new.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
